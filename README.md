# Increasing The Performance of Cognitively Inspired Sample-Efficient Language Models via Implicit Structure Building

Currently, the repository is basically a fork of BabyLM's baseline pretraining codes, later we will update with our experiments codes.
