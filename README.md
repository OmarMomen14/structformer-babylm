# Increasing The Performance of Cognitively Inspired Sample-Efficient Language Models via Implicit Structure Building

Currently, the repository is basically a fork of BabyLM's baseline pretraining codes, later we will update with our experiments codes.

# Models Checkpoints:

1. [transformer-base](https://huggingface.co/omarmomen/transformer_base_final_2)
2. [structformer_s1](https://huggingface.co/omarmomen/structformer_s1_final_with_pos)
3. [structformer_s2](https://huggingface.co/omarmomen/structformer_s2_final_with_pos)
4. [structroberta_s1](https://huggingface.co/omarmomen/structroberta_s1_final)
5. [structroberta_s2](https://huggingface.co/omarmomen/structroberta_s2_final)
6. [structroberta_s1`](https://huggingface.co/omarmomen/structroberta_sx_final)
7. [structroberta_s2`](https://huggingface.co/omarmomen/structroberta_sx2_final)
